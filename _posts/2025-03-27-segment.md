---
layout: post
title: "segment bitmap"
slug: segment
category: essay
---

## Segment Service

유저의 분류를 통하여 특정 유저군에게만 다른 행동을 하는 것은 복잡한 제품군을 만들 때
많은 도움이 된다. A 유저군에게만 광고를 할당하거나, 푸시를 보내거나, 혹은 어플리케이션에서
다른 행동을 할 수 있다. 유저의 세그먼트를 발라내는 것은 이런 저런 회사에서 내부적으로 많이들
지원하는 기능이다. 만약 내가 이런 기능을 만든다면 어떻게 만들었을지 한번 생각해보았다.

### query engine

segment를 분리하기 위하여는 유저군을 가져오는 쿼리가 필요하다. 버즈빌에서는 athena를 사용했었고,
다른 회사에서는 다른 데이터 분석 도구를 사용할 수도 있을 것이다. 당연히 매우 무거운 쿼리가
돌아갈 것이고, 몇십개의 테이블에서 몇억row를 뒤져서 몇천만의 유저를 타겟하는 쿼리가 만들어질 수 있다.

이런 데이터 분석 도구와의 결합은 매우 중요한 부분인데, 이번 구현에서는 이를 완전히 무시하겠다.

### request pattern

대부분의 요청은 한두명의 유저에 대하여 이루어 진다. '앨리스'라는 유저가 어떤 사람인가, 매일 들어오는 유저인가? 혹은 구매를 자주 하는 유저인가, 혹은 치킨을 최근 한달중에 일주일에 1번 이상 시킨 유저인지 보고싶기 때문이다.

요청에 대한 응답은 매우 빨라야하는데, 대부분의 요청이 몇ms안에 응답을 주기를 기대한다. 그래야지 다른 서비스들에서 부담없이 호출할 것이기에. 만약 10ms가 넘어가기 시작하면 서비스 개발자들은 이 요청이 무겁다고 느낄것이며, 100ms가 넘어간다면 도저히 느려서 사용할 수 없다고 느낄지도 모른다. 그러니 latency와 throuput은 매우 중요한 요소이다.

### update pattern, consistency

몇몇 consistency가 중요한 동작이 아니라면 이 segment는 천천히 업데이트되는 편이 낫다. 유저의 상태를 이벤트에 따라서 상태를 업데이트해야한다면 내가 버즈빌에서 구현했던 AST based State Machine을 구현하는게 좋다. 하지만 우리는 그정도까지의 정합성, 실시간성이 필요 없는 경우가 많다.

이번에 소개하는 구현의 경우에는 간단한 규칙이고, application에서 유저의 상태를 바꾸고자한다면 쉽게 바꿀 수 있는 구조이다. 물론 현재 상태를 계산하는데 많은 비용이 든다면 불가능하겠지만.

또한 유저의 상태를 바꾼다고 하더라도 vector clock이나 lamport timestamp, 혹은 distributed lock 없다면 consistency는 보장될 수 없음을 기억하자.

### only cache, not db

앞에서 이야기한 것들을 살펴보면 디비를 사용하기보다는 캐시를 사용하는게 자명하기에 대부분의 요청은 db가 아닌 cache, 흔히 redis에서 처리 된다. redis는 key / value 조회를 매우 빠르게 할 수 있는 좋은 도구이다.

redis는 key를 어떻게 설계할 것인지가 꽤나 중요해진다. 아래는 거의 대부분 segment 특성에 따라서 redis key pattern을 어떻게 설계할것인지를 다룬다.

### segment cardinality

segment는 여러가지 종류가 있을 수 있다. 가장 쉬운 분류는 세그먼트에 포함된 유저 집합의 크기이다. 1000명 미만이 포함된 매우 적은 집단, 만명부터 20만명정도가 포함된 보통 크기의 집단, 100만명정도가 포함된 집단이다. 이 집단별로 데이터를 다루는 패턴이 꽤 달라진다.

작은 집단의 경우에는 user id를 key로하고, segment id list를 value로 하면 유저 1-2명에 대하여 매우 빠르고, 메모리 효율적으로 조회할 수 있다. 만약 유저 1명당 확인해야하는 segment의 갯수가 많아지더라도 이 방식은 꽤나 효율적이다. redis 호출 한번으로 유저가 포함된 segment들을 전부 가져올 수 있다. 물론 application에서는 느려지겠지만, application은 scale out이 매우 쉬운 편이라서 대부분의 연산은 application에서 하는게 항상 옳다.

그러면 위 방식이 손해보는 경우는 없을까? 몇가지 있는데, segment 자체에 매우 많은 사람들이 포함되어있는 경우이다. 이때에는 redis의 key가 매우 많이 만들어지게 된다. 아마 대부분의 경우라면 그 서비스의 전체 인원을 대상이 redis에 각각 key로 올라갈 것이다.

> 전체 인원이 올라가는 것은 필연적인데, 교집합, 부분집합, 여집합등으로 만들게 된다면 어찌저찌 모든 유저가 전체 집합에 포함되는 것이다. 몇명정도는 아닐 수 있지만, 그들도 결국 언젠가는 어떠한 집단에 속하게 된다.

redis의 key가 많아지는게 문제일까? 일단 문제는 아니다. 당신의 회사가 돈이 매우 많고, 그것을 낭비해도 괜찮다면 이 상황을 축복하고, 회사의 돈을 마음껏 쓰도록 하자. 그렇지 않다면 최적화 할 수 있는 방법을 적용해보자.

redis는 key에 대하여 기본적인 오버헤드가 있다. 단순히 5자리 숫자를 저장하고싶다면 redis는 문자로 저장하니까 5바이트를 사용하고, dict의 entry를 생성하고, ttl이 있다면 ttl을 저장하기 위한 또다른 dict에 저장한다. 즉 실제 데이터는 매우 적지만 기본적인 오버헤드가 많아지는 것이다. 대강 50바이트라고 계산하면 되는데, 이는 redis 버전이나 구현에 따라 다르다.

50바이트의 오버헤드를 1비트로 줄일 수 있다면 어떨까? 1/400의 메모리 사용량으로 데이터를 저장할 수 있을 것이다. 이때 bitmap을 사용하게 된다.

```
[0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1]

user id가 1, 12, 15번이 SEGMENT에 포함된다면 위와 같이 표현할 수 있다.
```

이런 경우라면 segment당 고정된 size의 bitmap을 구현하는 것이라서 유저가 늘어난다고 메모리 사용량이 늘어나지 않고, 항상 고정된다. 이때 사용하는 메모리는 유저 1억명 기준으로 12MB정도이다. 간단한 수학을 해보자면 240000명 이상 있는 segment의 경우에는 bitmap을 사용하는게 메모리 효율적이다.

이렇게 구현한다면 redis key pattern이 바뀐 것이기에 redis key는 segment id가 된다. 유저가 2-3명이고, segment 1개에 대하여 확인하는 경우에는 빠르겠지만, segment 갯수가 늘어날때마다 redis 호출은 많아진다. 그래서 한명에 대하여 segment를 다양하게 확인해야하면 이 방식이 오히려 cpu는 비효율적이 된다. 일반적인 트레이드 오프인 것이다.

만약 메모리가 많다면 2가지 방법을 섞어서 구현할 수도 있다. 작은 segment는 user key에 저장하고, 큰 segment는 user key와 segment bitmap에 같이 저장하는 것이다. key가 두개로 나뉘면서 한 segment에서의 atomic한 연산은 추가적인 lua script와 같은 메커니즘이 필요해졌지만, 여전히 큰 오버헤드는 아닐 것이다.

이때 segment list를 조회할 때 큰 segment인지 아닌지에 따라서 조회하는 쿼리 패턴이 달라지기에 최적화 할 수 있는 hint가 있어야하는데, 이는 매우 적은 데이터이기에 로컬에서 들고있어도 아무런 무리가 없다.

### local cache와의 결합

나는 대부분의 요청하는 것들이 일부 컴포넌트에서 집중된다고 생각한다. 전체의 가짓수는 많겠지만, 빈도는 몇몇개가 압도적으로 많다는 것을 의미한다. 이는 우리 자본주의에서 부가 일부 상류층에 집중되는 것과 비슷하다. 그러니 엄청나게 큰 segment들이 한두명의 유저에 대하여 섞여서 요청이 들어오는 경우는 매우 적다.

상위 10개의 엄청나게 자주 사용되는 세그먼트 / 플레그 / 조건이 있을 것이다. 이러한 것들은 갯수가 얼마 되지 않기때문에 LOCAL에만 들고있어도 된다. redis까지 요청을 보내지 않고, 로컬에서 처리한다면 매우 빠르게 처리할 수 있고 어플리케이션의 리소스도 절감할 수 있다.

### bitmap 압축하기

bitmap을 segment로 사용한다면 density가 낮은 경우가 종종 있을 것이다. 그렇지 않더라도 12MB나 되는 큰 데이터를 redis에서 계속 가져온다면 redis의 network가 버티질 못할 것이다. 왜냐면 이 서비스에는 호출이 매우매우 많을 것이고, redis 호출도 많을 것이기에 네트워크의 스루풋이 중요해진다. NIC을 100G로 여러개 샤딩해서 사용한다면 조금 낫겠지만 그정도까지 하기보다는 압축을 하는 것이 더 빠르게 시도할 수 있는 것이다.

나는 bitmap을 압축하는걸 추천하지 않는데, application에서 bitmap을 가져올 때 압축 해제해야하는 것도 그렇지만, 전체 bitmap을 가져오지 않고록 유저 1명의 데이터를 확인할 수 있는 방법이 있다. redis의 BITFIELD 커맨드를 사용하면 특정 bit offset에서 바로 연산을 할 수 있다. 이 방법을 쓰면 네트워크도 아끼고, 빠르게 처리할 수 있다.

### large size memory allocation

압축은 권장하지 않고, 데이터를 한번에 가져오는 것도 권장하지 않는다면 사실상 get을 날리는 것인데 그 데이터가 string을 대상으로 하는것일 뿐이다.

여기에서 의문점이 들 수 있는데, "bitmap으로 바꾸었을 뿐이지 저장되어있는 데이터는 동일하다. bitmap은 MGET처럼 데이터를 한번에 가져오기만 하는게 아닌가? 이 경우에도 시간 복잡도를 O(1)이라고 할 수 있을까?" redis에 get을 여러번 날리는 것과, 커다란 keyvalue를 한번에 가져오는 것의 부하는 매우 다르다. redis는 대부분 user space의 cpu보다 sys(kernel) space의 cpu 사용량이 많다. 즉 network overhead가 크고, command를 파싱하는 것도 느리다. 그리고 tcp 관점에서는 어떻게든 전달해야하는 데이터라면 여러번 나눠서 하는게 아니라 한번에 처리하는 것이 더 효율적이다. stream 처리보다 batch가 스루풋이 높은 것은 흔한 패턴이다.

또한 하나의 데이터를 접근하는 것은 cache locality 측면에서 dict entry를 한번 조회하고, 그 다음에 socket에게 전체 데이터를 순차적으로 쓰기만 하면 되지만, 커맨드가 나눠지면 dict entry를 여러번 조회해야한다.

그러나 나는 bitmap을 쪼개는 것을 권장하고 싶다. 메모리 할당은 근본적으로 느리다. 이는 레디스에서도 동일하다. 매우 큰 bitmap이 존재하는 것은 막는게 좋다. 또한 application 관점에서도 매우 큰 객체를 생성하는 것은 부담이 된다. JVM에서는 힙 리전의 절반보다 큰 객체는 old gen에 바로 할당하고, 그 객체의 리스트를 따로 갖고있는다. (young gc가 실행될 때 정리해주긴한다.)

Go도 마찬가지로 32kb보다 큰 객체는 thread의 mcache에 할당되는게 아닌 mheap에 바로 할당된다. 이러한 객체들을 특별취급하는 런타임은 많다. 그렇기에 이런 객체가 늘어나면 여러 지표가 안좋아진다. Go는 객체의 memory 위치를 옮기지 않기에 문제가 더 심해질 수도 있다. 주기적으로 재기동이 필요한 서버는 아무도 원하지 않는다. 그러니 bitmap 각각을 작게 유지하자.

10만명정도를 포함하는 bitmap을 여러개 만들고, 그 순서마다 offset을 부여하여 하나의 큰 bitmap처럼 동작하게 하는걸 권장한다.

### bitmap 순회하기

종종 "특정 segment에 포함된 전체 유저를 알고싶어"와 같은 요청이 들어올 때도 bitmap은 강력하다. user id리스트를 내려준다면 매우 많은 요청을 날리거나, 많은 데이터를 주고 받아야하지만 bitmap은 한번만 받으면 된다. 그리고 bitmap의 첫번째 비트부터 마지막까지 순회하면 된다.

완전히 동일하지 않더라도 유저 1000-2000명 정도를 한번에 조회해야하는 경우가 종종 생긴다. 보통 이 경우는 유저의 갯수가 압도적으로 많고, 매우 적은 (2-10개) 갯수의 segment를 확인하는 경우이다. 연락처에서 내일 생일인 유저를 확인하는 경우가 그렇겠다.

이때도 user 갯수만큼 조회하는게 아닌 bitmap끼리 and 연산을 해서 빠르게 처리 가능하다. 네트워크를 최대한 타지 않는게 항상 좋다. 네트워크는 불필요한 대기를 만들고, cpu를 많이 쓰고, 네트워크도 쓰고, syscall도 늘어난다.

### 마무리

이러한 최적화는 큰 그림에서의 최적화를 보여주지만, network를 줄이면 줄일수록 cpu bound한 최적화들이 중요해진다. 다음에는 network가 없는 환경에서의 최적화들을 소개해보겠다.
